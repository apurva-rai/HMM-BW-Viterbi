# HMM-BW-Viterbi:

Barebones implementation of Hidden Markov Model trained with Baum-Welch algorithm. The Viterbi algorithm is used for prediction.
Install some general external dependancies used in ML.
I tried to optimize it using Numba jit but that was too tall an order for this project.

#Results:

Average run case:

![alt text](https://github.com/apurva-rai/HMM-BW-Viterbi/blob/master/img/test.png)

#References:

https://medium.com/analytics-vidhya/hidden-markov-model-part-1-of-the-hmm-series-3f7fea28a08

https://medium.com/analytics-vidhya/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86

https://docs.python.org/3/library/re.html

https://en.wikipedia.org/wiki/Hidden_Markov_model

https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm

https://stats.stackexchange.com/questions/31746/what-is-the-difference-between-the-forward-backward-and-viterbi-algorithms

https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string
